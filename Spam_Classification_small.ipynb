{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Business Objective: Create an Intelligent System to detect SPAM messages and filter them out to protect the system/mailbox/Inbox etc.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_xl = pd.read_excel(\"spam_msg.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Spam_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. \\nText FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 0845281007</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>URGENT! You have won a 1 week FREE membership in our å£100,000 Prize Jackpot! Txt the word: \\nCLAIM to No: 81010 T&amp;C www.dbuk.net LCCLTD POBOX 440...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Congrats! 1 year special cinema pass for 2 is yours. call 09061209465 now! C Suprman V, Matrix3, StarWars3,\\n etc all 4 FREE! bx420-ip4-5we. 150pm...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What you thinked about me. First time you saw me in class</td>\n",
       "      <td>non-spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>non-spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                    Text  \\\n",
       "0   Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. \\nText FA to 87121 to receive entry question(std txt rate)T&C's apply 0845281007   \n",
       "1  URGENT! You have won a 1 week FREE membership in our å£100,000 Prize Jackpot! Txt the word: \\nCLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 440...   \n",
       "2  Congrats! 1 year special cinema pass for 2 is yours. call 09061209465 now! C Suprman V, Matrix3, StarWars3,\\n etc all 4 FREE! bx420-ip4-5we. 150pm...   \n",
       "3                                                                                              What you thinked about me. First time you saw me in class   \n",
       "4                                                                          Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "\n",
       "  Spam_label  \n",
       "0       spam  \n",
       "1       spam  \n",
       "2       spam  \n",
       "3   non-spam  \n",
       "4   non-spam  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_xl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 2)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_xl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise : encode the response column \"Spam_label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.max_rows\", 1000)\n",
    "pd.set_option('max_colwidth',150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Spam_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. \\nText FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 0845281007</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>URGENT! You have won a 1 week FREE membership in our å£100,000 Prize Jackpot! Txt the word: \\nCLAIM to No: 81010 T&amp;C www.dbuk.net LCCLTD POBOX 440...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Congrats! 1 year special cinema pass for 2 is yours. call 09061209465 now! C Suprman V, Matrix3, StarWars3,\\n etc all 4 FREE! bx420-ip4-5we. 150pm...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What you thinked about me. First time you saw me in class</td>\n",
       "      <td>non-spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>non-spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                    Text  \\\n",
       "0   Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. \\nText FA to 87121 to receive entry question(std txt rate)T&C's apply 0845281007   \n",
       "1  URGENT! You have won a 1 week FREE membership in our å£100,000 Prize Jackpot! Txt the word: \\nCLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 440...   \n",
       "2  Congrats! 1 year special cinema pass for 2 is yours. call 09061209465 now! C Suprman V, Matrix3, StarWars3,\\n etc all 4 FREE! bx420-ip4-5we. 150pm...   \n",
       "3                                                                                              What you thinked about me. First time you saw me in class   \n",
       "4                                                                          Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "\n",
       "  Spam_label  \n",
       "0       spam  \n",
       "1       spam  \n",
       "2       spam  \n",
       "3   non-spam  \n",
       "4   non-spam  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_xl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Spam_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. \\nText FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 0845281007</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>URGENT! You have won a 1 week FREE membership in our å£100,000 Prize Jackpot! Txt the word: \\nCLAIM to No: 81010 T&amp;C www.dbuk.net LCCLTD POBOX 440...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Congrats! 1 year special cinema pass for 2 is yours. call 09061209465 now! C Suprman V, Matrix3, StarWars3,\\n etc all 4 FREE! bx420-ip4-5we. 150pm...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>URGENT! Your Mobile No. was awarded å£2000 Bonus Caller Prize on 5/9/03 This is our final try to contact U! \\nCall from Landline 09064019788 BOX42...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>You are a winner U have been specially selected 2 receive å£1000 or a 4* holiday (flights inc) speak to a live\\n operator 2 claim 0871277810910p/m...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Please call our customer service representative on 0800 169 6031 between 10am-9pm as you have WON a \\nguaranteed å£1000 cash or å£5000 prize</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Congratulations ur awarded 500 of CD vouchers or 125gift guaranteed &amp; Free entry 2 100 wkly draw txt MUSIC\\n to 87066 TnCs www.Ldew.com1win150ppmx...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>FREE RINGTONE text FIRST to 87131 for a poly or text GET to 87131 for a true tone! Help? 0845 2814032 16 after \\n1st free, tones are 3xå£150pw to ...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>FREE MESSAGE Activate your 500 FREE Text Messages by replying to this message with the word FREE For terms\\n &amp; conditions, visit www.07781482378.com</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>For ur chance to win a å£250 wkly shopping spree TXT: SHOP to 80878. T's&amp;C's www.txt-2-shop.com custcare 08715705022, 1x150p/wk</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                     Text  \\\n",
       "0    Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. \\nText FA to 87121 to receive entry question(std txt rate)T&C's apply 0845281007   \n",
       "1   URGENT! You have won a 1 week FREE membership in our å£100,000 Prize Jackpot! Txt the word: \\nCLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 440...   \n",
       "2   Congrats! 1 year special cinema pass for 2 is yours. call 09061209465 now! C Suprman V, Matrix3, StarWars3,\\n etc all 4 FREE! bx420-ip4-5we. 150pm...   \n",
       "8   URGENT! Your Mobile No. was awarded å£2000 Bonus Caller Prize on 5/9/03 This is our final try to contact U! \\nCall from Landline 09064019788 BOX42...   \n",
       "9   You are a winner U have been specially selected 2 receive å£1000 or a 4* holiday (flights inc) speak to a live\\n operator 2 claim 0871277810910p/m...   \n",
       "10           Please call our customer service representative on 0800 169 6031 between 10am-9pm as you have WON a \\nguaranteed å£1000 cash or å£5000 prize   \n",
       "14  Congratulations ur awarded 500 of CD vouchers or 125gift guaranteed & Free entry 2 100 wkly draw txt MUSIC\\n to 87066 TnCs www.Ldew.com1win150ppmx...   \n",
       "15  FREE RINGTONE text FIRST to 87131 for a poly or text GET to 87131 for a true tone! Help? 0845 2814032 16 after \\n1st free, tones are 3xå£150pw to ...   \n",
       "16  FREE MESSAGE Activate your 500 FREE Text Messages by replying to this message with the word FREE For terms\\n & conditions, visit www.07781482378.com    \n",
       "17                        For ur chance to win a å£250 wkly shopping spree TXT: SHOP to 80878. T's&C's www.txt-2-shop.com custcare 08715705022, 1x150p/wk   \n",
       "\n",
       "   Spam_label  \n",
       "0        spam  \n",
       "1        spam  \n",
       "2        spam  \n",
       "8        spam  \n",
       "9        spam  \n",
       "10       spam  \n",
       "14       spam  \n",
       "15       spam  \n",
       "16       spam  \n",
       "17       spam  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Sample spam messages\n",
    "spam_xl[spam_xl.Spam_label=='spam'].head(100) # spam msg\n",
    "#spam_xl[spam_xl.Spam_label=='non-spam'].head(100) # non-spam msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_xl['Spam_label']=spam_xl['Spam_label'].replace(['spam','non-spam'],[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Spam_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. \\nText FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 0845281007</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>URGENT! You have won a 1 week FREE membership in our å£100,000 Prize Jackpot! Txt the word: \\nCLAIM to No: 81010 T&amp;C www.dbuk.net LCCLTD POBOX 440...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Congrats! 1 year special cinema pass for 2 is yours. call 09061209465 now! C Suprman V, Matrix3, StarWars3,\\n etc all 4 FREE! bx420-ip4-5we. 150pm...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What you thinked about me. First time you saw me in class</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                    Text  \\\n",
       "0   Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. \\nText FA to 87121 to receive entry question(std txt rate)T&C's apply 0845281007   \n",
       "1  URGENT! You have won a 1 week FREE membership in our å£100,000 Prize Jackpot! Txt the word: \\nCLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 440...   \n",
       "2  Congrats! 1 year special cinema pass for 2 is yours. call 09061209465 now! C Suprman V, Matrix3, StarWars3,\\n etc all 4 FREE! bx420-ip4-5we. 150pm...   \n",
       "3                                                                                              What you thinked about me. First time you saw me in class   \n",
       "4                                                                          Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "\n",
       "   Spam_label  \n",
       "0           1  \n",
       "1           1  \n",
       "2           1  \n",
       "3           0  \n",
       "4           0  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_xl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spam_encode(input):\n",
    "    if input == 'spam':\n",
    "        return 1\n",
    "    if input == 'non-spam':\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_xl['spam_cd'] = spam_xl['Spam_label'].apply(spam_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Spam_label</th>\n",
       "      <th>spam_cd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. \\nText FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 0845281007</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>URGENT! You have won a 1 week FREE membership in our å£100,000 Prize Jackpot! Txt the word: \\nCLAIM to No: 81010 T&amp;C www.dbuk.net LCCLTD POBOX 440...</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Congrats! 1 year special cinema pass for 2 is yours. call 09061209465 now! C Suprman V, Matrix3, StarWars3,\\n etc all 4 FREE! bx420-ip4-5we. 150pm...</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What you thinked about me. First time you saw me in class</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                    Text  \\\n",
       "0   Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. \\nText FA to 87121 to receive entry question(std txt rate)T&C's apply 0845281007   \n",
       "1  URGENT! You have won a 1 week FREE membership in our å£100,000 Prize Jackpot! Txt the word: \\nCLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 440...   \n",
       "2  Congrats! 1 year special cinema pass for 2 is yours. call 09061209465 now! C Suprman V, Matrix3, StarWars3,\\n etc all 4 FREE! bx420-ip4-5we. 150pm...   \n",
       "3                                                                                              What you thinked about me. First time you saw me in class   \n",
       "4                                                                          Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "\n",
       "   Spam_label spam_cd  \n",
       "0           1    None  \n",
       "1           1    None  \n",
       "2           1    None  \n",
       "3           0    None  \n",
       "4           0    None  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_xl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text           0\n",
       "Spam_label     0\n",
       "spam_cd       18\n",
       "dtype: int64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_xl.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data pre-processing steps using reference\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk library installation & import\n",
    "\n",
    "# conda install -c anaconda nltk\n",
    "# import nltk\n",
    "# nltk.download ()    - to download nltk corpus and datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### NLTK Tokenizers\n",
    "from nltk import  sent_tokenize, word_tokenize, pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we are going to learn NLP concepts.',\n",
       " '1st concept in the NLP is tokenization.']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize text into sentences\n",
    "para = \"we are going to learn NLP concepts. 1st concept in the NLP is tokenization. \"\n",
    "sent_tokenize(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'very',\n",
       " 'simple',\n",
       " 'text',\n",
       " 'data',\n",
       " 'for',\n",
       " 'tokenizatoin',\n",
       " 'example']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize sentence into words\n",
    "text = \"This is a very simple text data for tokenizatoin example \"\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we', 'ca', \"n't\", 'learn', 'NLP', 'in', 'one', 'hour']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"we can't learn NLP in one hour\"\n",
    "word_tokenize(text)  # can't got broken into two words, wrongly tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(\"[\\w']+\") # treat contraction has whole word instead spliting them into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we', \"can't\", 'learn', 'NLP', 'in', 'one', 'hour']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"we can't learn NLP in one hour\"\n",
    "word_tokens = tokenizer.tokenize(text) \n",
    "word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we', \"can't\", 'learn', 'NLP', 'in', 'one', 'hour']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we', \"can't\", 'learn', 'nlp', 'in', 'one', 'hour']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word.lower() for word in text.split(\" \")] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exer : tokenize the first document from given spam_xl text data using NLTK word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exer : tokenize the first document from given spam_xl text data without using NLTK word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we', \"can't\", 'learn', 'nlp', 'in', 'one', 'hour']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word.lower() for word in word_tokens] # lower case "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### stop word (noises) removal\n",
    "- Any text that doesn't add relevance value to context of the text data is known as Noise or stop wrod.\n",
    "- Example :   is, am, are, was, were, of , that , in etc, social media hastags, punctuations.\n",
    "- In NLP, focus is more on important words by removing unimportant words (noises).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spam_xl['Text'][0].split(\" \")\n",
    "#word_tokenize(spam_xl['Text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english') \n",
    "# For english, 179 stop word list given in NLTK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = \"This is a very simple text data for tokenization example\"\n",
    "\n",
    "# Exercise : remove the stop words from the document using above stop_words list\n",
    "# (1) - using NLTK word_tokenize()\n",
    "# (2) - using split() method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['simple', 'text', 'data', 'tokenization', 'example']"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word.lower() for word in text_1.split(\" \") if word.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform into lowercase , 'This' to 'this', "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Customized words list\n",
    "- some words are very common in each domain and those words present in almost all the document\n",
    "- Ex 'patient', 'dr.', 'mcg' are very common in clinical text, so these could be treated as potential stop words\n",
    "- most frequent terms can be treated as distracting terms(stop words) \n",
    "- least frequent terms can be treated as distracting terms(stop words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words[0:10] # nltk stop word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words) # 179 stop words list given by NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'this' in stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise : remove stop words using customized stop words list custom_stop_words in healthcare domain\n",
    "text_1 = \"dr Shyam has successfully completed his nuerosurgery\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#['dr', 'shyam', 'successfully', 'completed', 'nuerosurgery']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stop_words = set(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'dr',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'mcg',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'patient',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# added three more stop words in the given list for Clinical Text domain\n",
    "custom_stop_words.update(['patient', 'dr', 'mcg'])  \n",
    "print(len(custom_stop_words)) # now total stop words list = 182\n",
    "custom_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [word.lower() for word in text_1.split(\" \") if word.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use customized word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spam_xl['Text'][0].split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[word for word in words_tokens if word not in stop_words]\n",
    "\n",
    "#text_1 = \"This is a very simple text data for tokenization example\"\n",
    "#[word for word in text_1.split(\" \") if word not in stop_words]\n",
    "#[word.lower() for word in text_1.split(\" \") if word.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# nltk stop word list\n",
    "#[word.lower() for word in text_1.split(\" \") if word.lower() not in stop_words] \n",
    "#[word.lower() for word in text_1.split(\" \") if word.lower() not in custom_stop_words] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'believ'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# - Stemming is a technique to remove affixes from a word, ending up with the stem. For example, the stem of cooking is cook.\n",
    "# Stemming (transformation)  --- Porter Stemmer, Lancaster Stemming algorithm\n",
    "from nltk.stem import PorterStemmer\n",
    "port_stemmer = PorterStemmer()\n",
    "port_stemmer.stem('believed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization :  belief\n",
      "stemming :  believ\n"
     ]
    }
   ],
   "source": [
    "# Lemmatizer looks meaning of the word while stemmer looks form of the word.\n",
    "# Lemmatization & stemming\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print('Lemmatization : ', lemmatizer.lemmatize('believes'))\n",
    "\n",
    "from nltk.stem import LancasterStemmer\n",
    "Lanc_stemmer = LancasterStemmer()\n",
    "print('stemming : ', Lanc_stemmer.stem('believed'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise : remove stop words and apply stemming in the below document\n",
    "text_1 = \"dr Shyam has successfully completed his nuerosurgery\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word normalization standardization \n",
    "dict = {'2day':'today' ,'hv':'have', 'ctc':'cost to company', 'add':'address', 'asap':'as soon as possible', 'f2f':'face to face' } \n",
    "\n",
    "input_text = '2day we hv a scheduled interview discussion at Raheja Mind Space Center'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'today'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict['2day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2day',\n",
       " 'we',\n",
       " 'hv',\n",
       " 'a',\n",
       " 'scheduled',\n",
       " 'interview',\n",
       " 'discussion',\n",
       " 'at',\n",
       " 'Raheja',\n",
       " 'Mind',\n",
       " 'Space',\n",
       " 'Center']"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2day',\n",
       " 'we',\n",
       " 'hv',\n",
       " 'a',\n",
       " 'scheduled',\n",
       " 'interview',\n",
       " 'discussion',\n",
       " 'at',\n",
       " 'Raheja',\n",
       " 'Mind',\n",
       " 'Space',\n",
       " 'Center']"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word for word in input_text.split()  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['today',\n",
       " 'we',\n",
       " 'have',\n",
       " 'a',\n",
       " 'scheduled',\n",
       " 'interview',\n",
       " 'discussion',\n",
       " 'at',\n",
       " 'raheja',\n",
       " 'mind',\n",
       " 'space',\n",
       " 'center']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = [dict[word.lower()] if word.lower() in dict else word.lower() for word in input_text.split()  ] \n",
    "input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_1 = [word.lower() for word in text_1.split(\" \") if word.lower() not in custom_stop_words ]\n",
    "# text_1 = [ port_stemmer.stem(word) for word in text_1] # 'completed'  to 'complete', 'successfully' to 'success' \n",
    "# text_1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Junk removal using Regex\n",
    "- Basics of Text and Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing junk words that starts with non-alphnumeric or numbers\n",
    "# token = re.sub(r'[\\W\\d]', \" \", token)  # \\W - matches with Non-Alphanumeric characters, \\d - matches with digits [0-9] \n",
    "\n",
    "input_text = '2day we hv a scheduled interview discussion at $1th road #t'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we', 'hv', 'a', 'scheduled', 'interview', 'discussion', 'at', 'road']"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word for word in input_text.split() if not re.search(r'[\\W\\d]', word) ] \n",
    "# \\W - matches with Non-Alphanumeric characters\n",
    "# \\d - matches with digits [0-9] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise - remove stop words, stemming, word standardization, apply regex to remove junk words \n",
    "text_1 =  '2day we hv a scheduled interview discussion at %1th road #t'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Pre-processing and Cleaning -  UDFS\n",
    "- Data cleaning, Transformation and word standardization udfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(row):\n",
    "    if row is None or row is '':\n",
    "        tokens = \"\"\n",
    "    else:\n",
    "        tokens = row.split(\" \")\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_reg_expressions(row):\n",
    "    tokens = []\n",
    "    try:\n",
    "        for token in row:\n",
    "            token = token.lower()\n",
    "            token = re.sub(r'[\\W\\d]', \" \", token)  # \\W - matches with Non-Alphanumeric characters, \\d - matches with digits [0-9] \n",
    "            tokens.append(token)\n",
    "    except:\n",
    "        token = \"\"\n",
    "        tokens.append(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# text_1 = '2day we hv a scheduled interview discussion at %1th road #t'\n",
    "# text_1 = [word.lower() for word in text_1.split(\" \") if word.lower() not in custom_stop_words ]\n",
    "# text_1 = [ port_stemmer.stem(word) for word in text_1]\n",
    "# text_1 = [dict[word.lower()] if word.lower() in dict else word.lower() for word in text_1  ]\n",
    "\n",
    "# import re\n",
    "# text_1 = [word for word in text_1 if not re.search(r'[\\W\\d]', word)]\n",
    "# text_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_word_removal(row):\n",
    "    token = [token for token in row if token not in stopwords]\n",
    "    token = filter(None, token)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_bag(data):\n",
    "    data = data.apply(tokenize)\n",
    "    data = data.apply(stop_word_removal)\n",
    "    data = data.apply(remove_reg_expressions)\n",
    "    \n",
    "    unique_tokens = []\n",
    "    single_tokens = []\n",
    "\n",
    "    for item in data:\n",
    "        for token in item:\n",
    "            if token in single_tokens:\n",
    "                if token not in unique_tokens:\n",
    "                    unique_tokens.append(token)\n",
    "            else:\n",
    "                single_tokens.append(token)\n",
    "    \n",
    "    df = pd.DataFrame(0, index = np.arange(len(data)), columns = unique_tokens)\n",
    "    \n",
    "    for i, item in enumerate(data):\n",
    "        for token in item:\n",
    "            if token in unique_tokens:\n",
    "                df.iloc[i][token] += 1    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bags_of_words  - Term Frequency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fa</th>\n",
       "      <th></th>\n",
       "      <th>entry</th>\n",
       "      <th></th>\n",
       "      <th>free</th>\n",
       "      <th>txt</th>\n",
       "      <th>apply</th>\n",
       "      <th>me</th>\n",
       "      <th>like</th>\n",
       "      <th>class</th>\n",
       "      <th>call</th>\n",
       "      <th>urgent</th>\n",
       "      <th>no</th>\n",
       "      <th>prize</th>\n",
       "      <th>final</th>\n",
       "      <th></th>\n",
       "      <th>you</th>\n",
       "      <th>receive</th>\n",
       "      <th>å</th>\n",
       "      <th>speak</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>later</th>\n",
       "      <th>i</th>\n",
       "      <th>u</th>\n",
       "      <th>are</th>\n",
       "      <th>did</th>\n",
       "      <th>awarded</th>\n",
       "      <th>wkly</th>\n",
       "      <th>first</th>\n",
       "      <th>text</th>\n",
       "      <th></th>\n",
       "      <th>st</th>\n",
       "      <th>free</th>\n",
       "      <th>message</th>\n",
       "      <th>for</th>\n",
       "      <th>ur</th>\n",
       "      <th>win</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fa         entry     free  txt  apply  me   like  class  call  urgent   \\\n",
       "0    2      2      2  1     1    1      1    0     0      0     0        0   \n",
       "1    0      1      0  1     1    1      1    0     0      0     0        1   \n",
       "2    0      0      0  3     0    0      0    0     0      0     1        0   \n",
       "3    0      0      0  0     0    0      0    1     0      1     0        0   \n",
       "4    0      0      0  0     0    0      0    1     2      0     0        0   \n",
       "5    0      0      0  0     0    0      0    0     0      0     0        0   \n",
       "6    0      0      0  2     1    0      0    0     0      1     0        0   \n",
       "7    0      0      0  0     0    0      0    0     0      0     1        0   \n",
       "8    0      0      0  0     0    0      0    0     0      0     0        1   \n",
       "9    0      0      0  2     0    0      0    0     0      0     0        0   \n",
       "10   0      0      0  0     0    0      0    0     0      0     1        0   \n",
       "11   0      0      0  0     0    0      0    0     0      0     0        0   \n",
       "12   0      0      0  0     0    0      0    0     0      0     0        0   \n",
       "13   0      0      0  4     0    0      0    0     0      0     0        0   \n",
       "14   0      1      1  2     1    1      0    0     0      0     0        0   \n",
       "15   0      2      0  0     1    0      0    0     0      0     0        0   \n",
       "16   0      0      0  1     3    0      0    0     0      0     0        0   \n",
       "17   0      0      0  0     0    0      0    0     0      0     0        0   \n",
       "\n",
       "    no   prize  final               you  receive  å       speak             \\\n",
       "0     0      0      1            0    0        1       0      0    0     0   \n",
       "1     1      1      0            0    1        0       0      0    0     0   \n",
       "2     0      0      0            1    0        0       0      0    1     0   \n",
       "3     0      0      0            0    0        0       0      0    0     0   \n",
       "4     0      0      0            0    0        0       0      1    0     0   \n",
       "5     0      0      0            0    0        0       0      0    0     0   \n",
       "6     0      0      0            0    0        0       0      0    0     0   \n",
       "7     0      0      0            0    0        0       0      0    0     0   \n",
       "8     1      1      1            1    0        0       1      0    0     0   \n",
       "9     0      0      0            0    1        1       1      1    0     0   \n",
       "10    0      1      0            0    0        0       2      0    1     2   \n",
       "11    0      0      0            0    0        0       0      0    0     0   \n",
       "12    0      0      0            0    0        0       0      0    1     0   \n",
       "13    0      0      0            0    0        0       0      0    0     0   \n",
       "14    0      0      0            0    0        0       0      0    2     0   \n",
       "15    0      0      0            0    0        0       0      0    0     1   \n",
       "16    0      0      0            0    0        0       0      0    1     0   \n",
       "17    0      0      0            0    0        0       0      0    0     0   \n",
       "\n",
       "    later   i  u  are  did  awarded  wkly  first  text        st  free   \\\n",
       "0        0  0  0    0    0        0     1      0     0   0     1      0   \n",
       "1        0  0  0    0    0        0     0      0     0   0     0      0   \n",
       "2        0  0  0    0    0        0     0      0     0   0     0      1   \n",
       "3        0  0  0    0    0        0     0      1     0   0     0      0   \n",
       "4        0  0  0    0    0        0     0      0     0   0     0      0   \n",
       "5        0  0  0    0    0        0     0      0     0   0     0      0   \n",
       "6        0  0  0    1    0        0     0      0     0   0     0      0   \n",
       "7        1  1  0    0    0        0     0      0     0   0     0      0   \n",
       "8        0  0  0    0    0        1     0      0     0   0     0      0   \n",
       "9        0  0  1    0    0        0     0      0     0   1     0      0   \n",
       "10       0  0  0    0    0        0     0      0     0   0     0      0   \n",
       "11       0  0  0    0    0        0     0      0     0   0     0      0   \n",
       "12       1  1  1    0    0        0     0      0     0   0     0      0   \n",
       "13       0  0  0    2    2        0     0      0     0   0     0      0   \n",
       "14       0  0  0    0    0        1     1      0     0   0     0      0   \n",
       "15       0  0  0    0    0        0     0      1     2   1     1      1   \n",
       "16       0  0  0    0    0        0     0      0     1   0     0      0   \n",
       "17       0  0  0    0    0        0     1      0     0   0     0      0   \n",
       "\n",
       "    message  for  ur  win          \n",
       "0         0    0   0    1       0  \n",
       "1         0    0   0    0       0  \n",
       "2         0    0   0    0       0  \n",
       "3         0    0   0    0       0  \n",
       "4         0    0   0    0       0  \n",
       "5         0    0   0    0       0  \n",
       "6         0    0   0    0       0  \n",
       "7         0    0   0    0       0  \n",
       "8         0    0   0    0       1  \n",
       "9         0    0   0    0       0  \n",
       "10        0    0   0    0       0  \n",
       "11        0    0   0    0       0  \n",
       "12        0    0   0    0       0  \n",
       "13        0    0   0    0       0  \n",
       "14        0    0   1    0       0  \n",
       "15        0    0   0    0       0  \n",
       "16        2    1   0    0       0  \n",
       "17        0    1   1    1       1  "
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = assemble_bag(spam_xl[\"Text\"])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 39)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape  # total 4981 unique features (words) got created for 5571 messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fa</th>\n",
       "      <th></th>\n",
       "      <th>entry</th>\n",
       "      <th></th>\n",
       "      <th>free</th>\n",
       "      <th>txt</th>\n",
       "      <th>apply</th>\n",
       "      <th>me</th>\n",
       "      <th>like</th>\n",
       "      <th>class</th>\n",
       "      <th>call</th>\n",
       "      <th>urgent</th>\n",
       "      <th>no</th>\n",
       "      <th>prize</th>\n",
       "      <th>final</th>\n",
       "      <th></th>\n",
       "      <th>you</th>\n",
       "      <th>receive</th>\n",
       "      <th>å</th>\n",
       "      <th>speak</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>later</th>\n",
       "      <th>i</th>\n",
       "      <th>u</th>\n",
       "      <th>are</th>\n",
       "      <th>did</th>\n",
       "      <th>awarded</th>\n",
       "      <th>wkly</th>\n",
       "      <th>first</th>\n",
       "      <th>text</th>\n",
       "      <th></th>\n",
       "      <th>st</th>\n",
       "      <th>free</th>\n",
       "      <th>message</th>\n",
       "      <th>for</th>\n",
       "      <th>ur</th>\n",
       "      <th>win</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fa         entry     free  txt  apply  me   like  class  call  urgent   \\\n",
       "0   2      2      2  1     1    1      1    0     0      0     0        0   \n",
       "1   0      1      0  1     1    1      1    0     0      0     0        1   \n",
       "2   0      0      0  3     0    0      0    0     0      0     1        0   \n",
       "3   0      0      0  0     0    0      0    1     0      1     0        0   \n",
       "4   0      0      0  0     0    0      0    1     2      0     0        0   \n",
       "\n",
       "   no   prize  final               you  receive  å       speak             \\\n",
       "0    0      0      1            0    0        1       0      0    0     0   \n",
       "1    1      1      0            0    1        0       0      0    0     0   \n",
       "2    0      0      0            1    0        0       0      0    1     0   \n",
       "3    0      0      0            0    0        0       0      0    0     0   \n",
       "4    0      0      0            0    0        0       0      1    0     0   \n",
       "\n",
       "   later   i  u  are  did  awarded  wkly  first  text        st  free   \\\n",
       "0       0  0  0    0    0        0     1      0     0   0     1      0   \n",
       "1       0  0  0    0    0        0     0      0     0   0     0      0   \n",
       "2       0  0  0    0    0        0     0      0     0   0     0      1   \n",
       "3       0  0  0    0    0        0     0      1     0   0     0      0   \n",
       "4       0  0  0    0    0        0     0      0     0   0     0      0   \n",
       "\n",
       "   message  for  ur  win          \n",
       "0        0    0   0    1       0  \n",
       "1        0    0   0    0       0  \n",
       "2        0    0   0    0       0  \n",
       "3        0    0   0    0       0  \n",
       "4        0    0   0    0       0  "
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if any feautres names as space \"\" or \" \", remove them\n",
    "x.drop(labels = [' ', '  ','   ', '    ', '      ', '     '], inplace=True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 33)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape # 4 column with named as spaces got removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fa</th>\n",
       "      <th>entry</th>\n",
       "      <th>free</th>\n",
       "      <th>txt</th>\n",
       "      <th>apply</th>\n",
       "      <th>me</th>\n",
       "      <th>like</th>\n",
       "      <th>class</th>\n",
       "      <th>call</th>\n",
       "      <th>urgent</th>\n",
       "      <th>no</th>\n",
       "      <th>prize</th>\n",
       "      <th>final</th>\n",
       "      <th></th>\n",
       "      <th>you</th>\n",
       "      <th>receive</th>\n",
       "      <th>å</th>\n",
       "      <th>speak</th>\n",
       "      <th>later</th>\n",
       "      <th>i</th>\n",
       "      <th>u</th>\n",
       "      <th>are</th>\n",
       "      <th>did</th>\n",
       "      <th>awarded</th>\n",
       "      <th>wkly</th>\n",
       "      <th>first</th>\n",
       "      <th>text</th>\n",
       "      <th>st</th>\n",
       "      <th>free</th>\n",
       "      <th>message</th>\n",
       "      <th>for</th>\n",
       "      <th>ur</th>\n",
       "      <th>win</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fa  entry  free  txt  apply  me   like  class  call  urgent   no   prize  \\\n",
       "0   2      2     1    1      1    0     0      0     0        0    0      0   \n",
       "1   0      0     1    1      1    0     0      0     0        1    1      1   \n",
       "2   0      0     0    0      0    0     0      0     1        0    0      0   \n",
       "3   0      0     0    0      0    1     0      1     0        0    0      0   \n",
       "4   0      0     0    0      0    1     2      0     0        0    0      0   \n",
       "\n",
       "   final               you  receive  å       speak  later   i  u  are  did  \\\n",
       "0      1            0    0        1       0      0       0  0  0    0    0   \n",
       "1      0            0    1        0       0      0       0  0  0    0    0   \n",
       "2      0            1    0        0       0      0       0  0  0    0    0   \n",
       "3      0            0    0        0       0      0       0  0  0    0    0   \n",
       "4      0            0    0        0       0      1       0  0  0    0    0   \n",
       "\n",
       "   awarded  wkly  first  text    st  free   message  for  ur  win  \n",
       "0        0     1      0     0     1      0        0    0   0    1  \n",
       "1        0     0      0     0     0      0        0    0   0    0  \n",
       "2        0     0      0     0     0      1        0    0   0    0  \n",
       "3        0     0      1     0     0      0        0    0   0    0  \n",
       "4        0     0      0     0     0      0        0    0   0    0  "
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.head()  # intput Feature ( BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. \\nText FA to 87121 to receive entry question(std txt rate)T&C's apply 0845281007\""
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_xl[\"Text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if word 'the'  exists in a stopword list\n",
    "stopwords.count('the') # it has one occurence of 'the'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entry</th>\n",
       "      <th>free</th>\n",
       "      <th>txt</th>\n",
       "      <th>apply</th>\n",
       "      <th>me</th>\n",
       "      <th>like</th>\n",
       "      <th>class</th>\n",
       "      <th>call</th>\n",
       "      <th>urgent</th>\n",
       "      <th>no</th>\n",
       "      <th>prize</th>\n",
       "      <th>final</th>\n",
       "      <th></th>\n",
       "      <th>you</th>\n",
       "      <th>receive</th>\n",
       "      <th>å</th>\n",
       "      <th>speak</th>\n",
       "      <th>later</th>\n",
       "      <th>did</th>\n",
       "      <th>awarded</th>\n",
       "      <th>wkly</th>\n",
       "      <th>first</th>\n",
       "      <th>text</th>\n",
       "      <th>st</th>\n",
       "      <th>free</th>\n",
       "      <th>message</th>\n",
       "      <th>for</th>\n",
       "      <th>ur</th>\n",
       "      <th>win</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   entry  free  txt  apply  me   like  class  call  urgent   no   prize  \\\n",
       "0      2     1    1      1    0     0      0     0        0    0      0   \n",
       "1      0     1    1      1    0     0      0     0        1    1      1   \n",
       "2      0     0    0      0    0     0      0     1        0    0      0   \n",
       "3      0     0    0      0    1     0      1     0        0    0      0   \n",
       "4      0     0    0      0    1     2      0     0        0    0      0   \n",
       "\n",
       "   final               you  receive  å       speak  later   did  awarded  \\\n",
       "0      1            0    0        1       0      0       0    0        0   \n",
       "1      0            0    1        0       0      0       0    0        0   \n",
       "2      0            1    0        0       0      0       0    0        0   \n",
       "3      0            0    0        0       0      0       0    0        0   \n",
       "4      0            0    0        0       0      1       0    0        0   \n",
       "\n",
       "   wkly  first  text    st  free   message  for  ur  win  \n",
       "0     1      0     0     1      0        0    0   0    1  \n",
       "1     0      0     0     0      0        0    0   0    0  \n",
       "2     0      0     0     0      1        0    0   0    0  \n",
       "3     0      1     0     0      0        0    0   0    0  \n",
       "4     0      0     0     0      0        0    0   0    0  "
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create some customized stop-word lists and remove those features from the BOW \n",
    "x.drop(labels = ['i', 'fa', 'u', 'are'], inplace=True, axis = 1)\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 29)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     None\n",
       "1     None\n",
       "2     None\n",
       "3     None\n",
       "4     None\n",
       "5     None\n",
       "6     None\n",
       "7     None\n",
       "8     None\n",
       "9     None\n",
       "10    None\n",
       "11    None\n",
       "12    None\n",
       "13    None\n",
       "14    None\n",
       "15    None\n",
       "16    None\n",
       "17    None\n",
       "Name: spam_cd, dtype: object"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output features\n",
    "y = spam_xl[\"spam_cd\"]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 29)\n",
      "(18,)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install wordcloud from anaconda prompt\n",
    "# conda install -c conda-forge wordcloud\n",
    "\n",
    "# import numpy as np\n",
    "# from PIL import Image\n",
    "# from wordcloud import WordCloud, ImageColorGenerator\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# % matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Spam_label</th>\n",
       "      <th>spam_cd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. \\nText FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 0845281007</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>URGENT! You have won a 1 week FREE membership in our å£100,000 Prize Jackpot! Txt the word: \\nCLAIM to No: 81010 T&amp;C www.dbuk.net LCCLTD POBOX 440...</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                    Text  \\\n",
       "0   Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. \\nText FA to 87121 to receive entry question(std txt rate)T&C's apply 0845281007   \n",
       "1  URGENT! You have won a 1 week FREE membership in our å£100,000 Prize Jackpot! Txt the word: \\nCLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 440...   \n",
       "\n",
       "   Spam_label spam_cd  \n",
       "0           1    None  \n",
       "1           1    None  "
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summary statistics of spam vs non-spam\n",
    "spam_xl.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: spam_cd, dtype: int64)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_xl.spam_cd.value_counts()  # non-spam = 8 (40%), spam=10 (60%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: Text, dtype: object)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = spam_xl[spam_xl.spam_cd == 1].Text # one message\n",
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wc = WordCloud().generate(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # display the generated text world cloud image \n",
    "# plt.imshow(wc, interpolation='bilinear')\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spam message patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Spam message patterns\n",
    "# txt = \" \".join(i for i in spam_xl[spam_xl.spam_cd == 1].Text)  \n",
    "\n",
    "# # Generate a word cloud image for spam messages\n",
    "# wc = WordCloud(stopwords=stopwords, background_color=\"white\").generate(txt)\n",
    "\n",
    "# # display the image \n",
    "# plt.imshow(wc, interpolation='bilinear')\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Machine learning Modeling using Bags-of-words features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entry</th>\n",
       "      <th>free</th>\n",
       "      <th>txt</th>\n",
       "      <th>apply</th>\n",
       "      <th>me</th>\n",
       "      <th>like</th>\n",
       "      <th>class</th>\n",
       "      <th>call</th>\n",
       "      <th>urgent</th>\n",
       "      <th>no</th>\n",
       "      <th>prize</th>\n",
       "      <th>final</th>\n",
       "      <th></th>\n",
       "      <th>you</th>\n",
       "      <th>receive</th>\n",
       "      <th>å</th>\n",
       "      <th>speak</th>\n",
       "      <th>later</th>\n",
       "      <th>did</th>\n",
       "      <th>awarded</th>\n",
       "      <th>wkly</th>\n",
       "      <th>first</th>\n",
       "      <th>text</th>\n",
       "      <th>st</th>\n",
       "      <th>free</th>\n",
       "      <th>message</th>\n",
       "      <th>for</th>\n",
       "      <th>ur</th>\n",
       "      <th>win</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   entry  free  txt  apply  me   like  class  call  urgent   no   prize  \\\n",
       "0      2     1    1      1    0     0      0     0        0    0      0   \n",
       "1      0     1    1      1    0     0      0     0        1    1      1   \n",
       "2      0     0    0      0    0     0      0     1        0    0      0   \n",
       "3      0     0    0      0    1     0      1     0        0    0      0   \n",
       "4      0     0    0      0    1     2      0     0        0    0      0   \n",
       "\n",
       "   final               you  receive  å       speak  later   did  awarded  \\\n",
       "0      1            0    0        1       0      0       0    0        0   \n",
       "1      0            0    1        0       0      0       0    0        0   \n",
       "2      0            1    0        0       0      0       0    0        0   \n",
       "3      0            0    0        0       0      0       0    0        0   \n",
       "4      0            0    0        0       0      1       0    0        0   \n",
       "\n",
       "   wkly  first  text    st  free   message  for  ur  win  \n",
       "0     1      0     0     1      0        0    0   0    1  \n",
       "1     0      0     0     0      0        0    0   0    0  \n",
       "2     0      0     0     0      1        0    0   0    0  \n",
       "3     0      1     0     0      0        0    0   0    0  \n",
       "4     0      0     0     0      0        0    0   0    0  "
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train_Test Splt for Machine Learning Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=.4, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 29)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 29)\n",
      "(8,)\n"
     ]
    }
   ],
   "source": [
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: 'unknown'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-194-e7c1429d7e37>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlogreg_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mlogreg_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0my_test_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogreg_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy\"\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1531\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype, order=\"C\",\n\u001b[0;32m   1532\u001b[0m                          accept_large_sparse=solver != 'liblinear')\n\u001b[1;32m-> 1533\u001b[1;33m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1534\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1535\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\multiclass.py\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    167\u001b[0m     if y_type not in ['binary', 'multiclass', 'multiclass-multioutput',\n\u001b[0;32m    168\u001b[0m                       'multilabel-indicator', 'multilabel-sequences']:\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Unknown label type: %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown label type: 'unknown'"
     ]
    }
   ],
   "source": [
    "# Multinomial Logistic Regress\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg_model = LogisticRegression()\n",
    "logreg_model.fit(x_train, y_train)\n",
    "y_test_pred = logreg_model.predict(x_test)\n",
    "print(\"Accuracy\",  metrics.accuracy_score(y_test, y_test_pred))\n",
    "print(\"Precision\", metrics.precision_score(y_test, y_test_pred))\n",
    "print(\"Sensitivity\", metrics.recall_score(y_test, y_test_pred))\n",
    "\n",
    "# Resp-label = 1 => Spam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_df = pd.DataFrame()\n",
    "comp_df['spam_actual'] = y_test\n",
    "comp_df['spam_pred']= y_test_pred\n",
    "\n",
    "comp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 150)\n",
    "spam_xl.iloc[[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.iloc[[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Model to detect if spam or genuine message\n",
    "logreg_model.predict(x_test.iloc[[3]])  # predicted as Spam (1 = spam, 0 = non-spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts() #` 60% of Spam messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial Naive Bayes Model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "NB_model = MultinomialNB()\n",
    "NB_model.fit(x_train, y_train)\n",
    "y_test_pred = NB_model.predict(x_test)\n",
    "print(\"Accuracy\",  metrics.accuracy_score(y_test, y_test_pred))\n",
    "print(\"Precision\", metrics.precision_score(y_test, y_test_pred))\n",
    "print(\"Sensitivity\", metrics.recall_score(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_df = pd.DataFrame()\n",
    "comp_df['spam_actual'] = y_test\n",
    "comp_df['spam_pred']= y_test_pred\n",
    "\n",
    "comp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_xl.iloc[[9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=50)\n",
    "rf_model.fit(x_train, y_train)\n",
    "y_test_pred = rf_model.predict(x_test)\n",
    "print(\"Accuracy\",  metrics.accuracy_score(y_test, y_test_pred))\n",
    "print(\"Precision\", metrics.precision_score(y_test, y_test_pred))\n",
    "print(\"Sensitivity\", metrics.recall_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### BOW (Bag of words) - TERM-Frequency using CountVectorizer() from sklearn.feature_extraction.text\n",
    "- We call vectorization the general process of turning a collection of text documents into numerical feature vectors.\n",
    "- Following the example of Vectorization (TF - term frequency) using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bags_of_words  - Term Frequency  (Scikit-Learn)\n",
    "- In order to address this, scikit-learn provides utilities for the most common ways to extract numerical features from text content, namely:\n",
    "- **tokenizing** strings and giving an integer id for each possible token, for instance by using white-spaces and punctuation as token separators.\n",
    "- **counting** the occurrences of tokens in each document.\n",
    "- **normalizing and weighting with diminishing importance** tokens that occur in the majority of samples / documents.\n",
    "- A corpus of documents can thus be represented by a matrix with one row per document and one column per token (e.g. word) occurring in the corpus\n",
    "- We call **vectorization** the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the Bag of Words or “Bag of n-grams” representation. \n",
    "- Documents are described by word occurrences while **completely ignoring the relative position information of the words** in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TF-IDF Based Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer :  implements both tokenization and occurrence counting in a single class:\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " corpus = [ 'This is the first document document.', \n",
    "            'This is the second second document.',\n",
    "            'And the third one.',\n",
    "            'Is this the first first document?' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(corpus) \n",
    "\n",
    "# The default configuration tokenizes the string by extracting words of at least 2 letters. \n",
    "# The specific function that does this step can be requested explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names() # words as feature names\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF Matrix calculated \n",
    "X.toarray() # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_values = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "tf_features = pd.DataFrame(tf_values, columns=feature_names)\n",
    "tf_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  corpus = [ 'This is the first document document.', \n",
    "#             'This is the second second document.',\n",
    "#             'And the third one.',\n",
    "#             'Is this the first first document?' ]\n",
    "    \n",
    "# create a vocabulary of words, \n",
    "# max_df=.85 => ignore words that appear in 85% of documents\n",
    "# min_df=.26  => ignore words that appear less than 25% of documents\n",
    "# max_features=10 => consider the top max_features ordered by term frequency across the corpus\n",
    "# eliminate stop words\n",
    "# limit our vocabulary size to 10\n",
    "\n",
    "vectorizer=CountVectorizer(max_df=.85, stop_words=stopwords, max_features=10 ) # max_df=.8 => 5 features, .6 => 4 features\n",
    "#vectorizer=CountVectorizer(min_df=.26, stop_words=stopwords, max_features=10 ) # min_df=.3 => 5 features, .6 => 4 features\n",
    "\n",
    "#vectorizer=CountVectorizer(ngram_range=(2,2),stop_words=stopwords, max_features=10 ) # ngram_range=(2,2) - bigram feature, tri gram \n",
    "\n",
    "x = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# bow_temp_tf_df = pd.DataFrame(x.toarray(), columns=vectorizer.get_feature_names())\n",
    "# print(bow_temp_tf_df.shape)\n",
    "# bow_temp_tf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a BOW feature Term-frequency for new documents \n",
    "# Hence words that were not seen in the training corpus will be completely ignored in future calls to the transform method:\n",
    "new_doc = [\"This is the fifth document only\", \"This is a sixth document only\"] # corpus is the list of documents\n",
    "vectorizer.transform(new_doc).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the tf-model trained from previous corpus, transform the new text documents into TF matrix\n",
    "tf_new_data = pd.DataFrame(vectorizer.transform(new_doc).toarray(), columns=vectorizer.get_feature_names())\n",
    "tf_new_data\n",
    "\n",
    "# New text docuemnt \"\"This is the fifth document only\", has one new feature 'fifth' that got ignored as this word was not there\n",
    "#                   in the training corpus data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Skelearn vectorizer (BOW): Convert a collection of text documents to a matrix of token counts\n",
    "\n",
    "- **sklearn.feature_extraction.text.CountVectorizer**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TF vectorization (BOW creation) from raw input text data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While cv.fit(...) would only create the vocabulary,\n",
    "# cv.fit_transform(...) creates the vocabulary and returns a term-document matrix \n",
    "# With this, each column in the matrix represents a word in the vocabulary while each row represents the\n",
    "# document in our dataset where the values in this case are the word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_xl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_xl.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the text data \n",
    "temp_data_old = spam_xl[\"Text\"][0:2].tolist()\n",
    "temp_data_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a vocabulary of words, \n",
    "# eliminate stop words\n",
    "# limit our vocabulary size to 10,00\n",
    "\n",
    "vectorizer=CountVectorizer(stop_words=stopwords, max_features=1000 )\n",
    "#vectorizer=CountVectorizer(stop_words=stopwords, max_features=1000 , token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\n",
    "x = vectorizer.fit_transform(temp_data_old)\n",
    "\n",
    "bow_tf_df = pd.DataFrame(x.toarray(), columns=vectorizer.get_feature_names())\n",
    "bow_tf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a vocabulary of words, \n",
    "# ignore words that appear in 85% of documents\n",
    "# eliminate stop words\n",
    "# limit our vocabulary size to 10,000\n",
    "# added the regex of token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,} to select the words started with alphanumeric only\n",
    "\n",
    "vectorizer=CountVectorizer(max_df=.85, stop_words=stopwords, max_features=1000,token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}' )\n",
    "x = vectorizer.fit_transform(temp_data_old)\n",
    "\n",
    "bow_tf_df = pd.DataFrame(x.toarray(), columns=vectorizer.get_feature_names())\n",
    "print(bow_tf_df.shape)\n",
    "bow_tf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_tf_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply count vectorizer on entire text data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_df=.85, stop_words=stopwords, max_features=1000,token_pattern='[a-zA-Z][a-zA-Z]{2,}')\n",
    "x_word_count_vec = vectorizer.fit_transform(spam_xl[\"Text\"])  # final word_count_vector\n",
    "\n",
    "x_bow_tf_df = pd.DataFrame(x_word_count_vec.toarray(), columns=vectorizer.get_feature_names())\n",
    "print(x_bow_tf_df.shape)\n",
    "x_bow_tf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# created bags of words and saved into sparse matrix row format\n",
    "x_word_count_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_word_count_vec.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### use sklearn TF-BOW to build the model to find the pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_xl.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x_bow_tf_df.copy()\n",
    "y = spam_xl[\"spam_cd\"]  \n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train_Test Splt for Machine Learning Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=.4, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial Logistic Regress\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg_model = LogisticRegression()\n",
    "logreg_model.fit(x_train, y_train)\n",
    "y_test_pred = logreg_model.predict(x_test)\n",
    "print(\"Accuracy\",  metrics.accuracy_score(y_test, y_test_pred))\n",
    "print(\"Precision\", metrics.precision_score(y_test, y_test_pred))\n",
    "print(\"Sensitivity\", metrics.recall_score(y_test, y_test_pred))\n",
    "\n",
    "# Resp-label = 1 => Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparision Actuala vs prediction\n",
    "\n",
    "comp_df = pd.DataFrame()\n",
    "comp_df['text_data'] = spam_xl[\"Text\"][x_test.index]\n",
    "comp_df['prediction']   = y_test_pred\n",
    "comp_df['actual']   = y_test\n",
    "\n",
    "comp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect the spam label for new message \"I'm leaving my house now... \"\n",
    "#new_doc = [\"You are a winner U have been specially selected 2 receive å£1000 cash or a 4* holiday (flights inc) speak to a live operator 2 claim 0871277810810\"]\n",
    "new_doc = [\"I'm leaving my house now... \", \"You are a winner U have been specially selected 2 receive å£1000 cash or a 4* holiday (flights inc) speak to a live operator 2 claim 0871277810810\"]\n",
    "\n",
    "tf_new_data = pd.DataFrame(vectorizer.transform(new_doc).toarray(), columns=vectorizer.get_feature_names())\n",
    "tf_new_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new_pred = logreg_model.predict(tf_new_data)  \n",
    "y_new_pred  # predicted as non-spam ( spam =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TF-IDF\n",
    "\n",
    "- - **Tf-idf makes rare words more prominent and effectively ignores common words.**\n",
    "- In a large text corpus, some words will be very present (e.g. “the”, “a”, “is” in English hence carrying very little meaningful information about the actual contents of the document.\n",
    "- If we were to feed the direct count data directly to a classifier those very frequent terms would shadow the frequencies of rarer yet more interesting terms.\n",
    "- In order to re-weight the count features into floating point values suitable for usage by a classifier it is very common to use the tf–idf transform.\n",
    "-  tfidf_transformer.transform(...). This generates a vector of tf-idf scores.\n",
    "- Next, we sort the words in the vector in descending order of tf-idf values and then iterate over to extract the top-n keywords.\n",
    "\n",
    "- we will defeat the whole purpose of IDF weighting if its not based on a large corpora as **(a) your vocabulary becomes too small and (b) you have limited ability to observe the behavior of words that you do know about.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tf means term-frequency while tf–idf means term-frequency times inverse document-frequency: \n",
    "# tf-idf(t,d) = tf(t,d) * idf(t)\n",
    "# term-frequency(tf) - the number of times a term occurs in a given document\n",
    "\n",
    "# inverse-document-frequency(idf) \n",
    "# idf(t) = log((1+nd)/(1+df(d,t)) + 1\n",
    "# nd : total number of documents\n",
    "# df(d,t) : is the number of documents that contain t\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### TF-IDF \n",
    "# after creating the word_count_vector, we need to crate TF-IDF matrix on top of created above \"x_word_count_vec\"\n",
    "# by using tfidf_transformer.fit(...)\n",
    "\n",
    "# temp_small-data\n",
    "temp_data_old = spam_xl['Text'][0:3].tolist()\n",
    "\n",
    "# create the word_count_vec \n",
    "vectorizer = CountVectorizer(max_df=0.85,stop_words=stopwords, max_features=10000, token_pattern='[a-zA-Z][a-zA-Z]{4,8}')\n",
    "x_temp_word_count_vec = vectorizer.fit_transform(temp_data_old)\n",
    "\n",
    "\n",
    "# view the word_count_vector created on this small data\n",
    "bow_tf_df = pd.DataFrame(x_temp_word_count_vec.toarray(), columns=vectorizer.get_feature_names())\n",
    "bow_tf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF - now create tf-idf on top of above x_temp_word_count_vec\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "x_tf_idf_wcv = tfidf_transformer.fit_transform(x_temp_word_count_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tf_idf_wcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the tf-idf word_count_vector\n",
    "bow_tf_idf = pd.DataFrame(x_tf_idf_wcv.toarray(), columns=vectorizer.get_feature_names())\n",
    "bow_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the word_count_vector created on this small data\n",
    "bow_tf_df = pd.DataFrame(x_temp_word_count_vec.toarray(), columns=vectorizer.get_feature_names())\n",
    "bow_tf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF creation using TfidfVectorizer()  \n",
    "\n",
    "# #### TF-IDF on spam datasets \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer_idf = TfidfVectorizer(max_df=0.85,stop_words=stopwords, max_features=10000, token_pattern='[a-zA-Z][a-zA-Z]{2,}')\n",
    "\n",
    "x_tf_idf = vectorizer_idf.fit_transform(spam_xl[\"Text\"])  # now we can see it has created 8535 features using one step TfidfVectorizer\n",
    "x_tf_idf\n",
    "\n",
    "# view the tf-idf word_count_vector\n",
    "x_bow_tf_idf = pd.DataFrame(x_tf_idf.toarray(), columns=vectorizer_idf.get_feature_names())\n",
    "print(x_bow_tf_idf.shape)\n",
    "x_bow_tf_idf.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Classsfication Model using TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x_bow_tf_idf.copy()\n",
    "y = spam_xl[\"spam_cd\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train_Test Splt for Machine Learning Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=.4, random_state=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial Logistic Regress\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg_model = LogisticRegression()\n",
    "logreg_model.fit(x_train, y_train)\n",
    "y_test_pred = logreg_model.predict(x_test)\n",
    "print(\"Accuracy\",  metrics.accuracy_score(y_test, y_test_pred))\n",
    "print(\"Precision\", metrics.precision_score(y_test, y_test_pred))\n",
    "print(\"Sensitivity\", metrics.recall_score(y_test, y_test_pred))\n",
    "\n",
    "# Resp-label = 1 => Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect the spam label for new message \"I'm leaving my house now... \"\n",
    "#new_doc = [\"I'm leaving my house now... \"]\n",
    "new_doc = [\"I'm leaving my house now... \", \"You are a winner U have been specially selected 2 receive å£1000 cash or a 4* holiday (flights inc) speak to a live operator 2 claim 0871277810810\"]\n",
    "\n",
    "tf_new_data = pd.DataFrame(vectorizer_idf.transform(new_doc).toarray(), columns=vectorizer_idf.get_feature_names())\n",
    "tf_new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the\n",
    "y_new_pred = logreg_model.predict(tf_new_data)  \n",
    "y_new_pred  # predicted as non-spam ( spam =0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TF-IDF Feature Importance selection - Chisq Test\n",
    "- Apply the chisq test to identify the important tf-idf features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Tuning the Max Features( words)** :  'vect__max_features': (None, 5000, 10000, 50000),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tuning the unigram and bigram words features 'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A collection of unigrams (what bag of words is) cannot capture phrases and multi-word expressions, \n",
    "# effectively disregarding any word order dependence.\n",
    "\n",
    "# N-grams to the rescue! Instead of building a simple collection of unigrams (n=1), \n",
    "# one might prefer a collection of bigrams (n=2), where occurrences of pairs of consecutive words are counted.\n",
    "\n",
    "# Example:\n",
    "# a 2-gram (or bigram) is a two-word sequence of words like “please turn”, “turn your”, or ”your homework”, \n",
    "# a 3-gram (or trigram) is a three-word sequence of words like “please turn your”, or “turn your homework”\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unigram & bi-gram () \n",
    "\n",
    "# temp_small-data\n",
    "temp_data_old = spam_xl['Text'][0:3].tolist()\n",
    "\n",
    "# create the word_count_vec \n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2), max_df=0.85,stop_words=stopwords, max_features=10000, token_pattern='[a-zA-Z][a-zA-Z]{2,}')\n",
    "x_temp_word_count_vec = vectorizer.fit_transform(temp_data_old)\n",
    "\n",
    "\n",
    "# view the word_count_vector created on this small data\n",
    "bow_tf_df = pd.DataFrame(x_temp_word_count_vec.toarray(), columns=vectorizer.get_feature_names())\n",
    "bow_tf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additionally, the bag of words model doesn’t account for potential misspellings or word derivations.\n",
    "\n",
    "# One might alternatively consider a collection of character n-grams, a representation resilient against misspellings and derivations.\n",
    "\n",
    "# For such languages it can increase both the predictive accuracy and convergence speed of classifiers \n",
    "# trained using such features while retaining the robustness with regards to misspellings and word derivations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, let’s say we’re dealing with a corpus of two documents: ['words', 'wprds']. \n",
    "# The second document contains a misspelling of the word ‘words’. A simple bag of words representation would \n",
    "# consider these two as very distinct documents, differing in both of the two possible features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(2, 2))\n",
    "counts = ngram_vectorizer.fit_transform(['words', 'wprds'])\n",
    "\n",
    "# view the tf-idf word_count_vector\n",
    "x_bow_tf = pd.DataFrame(counts.toarray(), columns=ngram_vectorizer.get_feature_names())\n",
    "print(x_bow_tf.shape)\n",
    "x_bow_tf.head(10)\n",
    "\n",
    "#  A character 2-gram representation, however, would find the documents matching in 4 out of 8 features,\n",
    "#  which may help the preferred classifier decide better:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### pipeline for text feature extraction and evaluation using Gridsearch Cross Validation \n",
    "-  Define a pipeline combining a text feature extractor with a simple classifier\n",
    "-  **Tuning the Max Features( words)** :  'vect__max_features': (None, 5000, 10000, 50000),\n",
    "-  **Tuning the Unigram or bigram words** :   'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "-  ** we need to tune the ML Algorithm Hyper paramter ** for each occurences in the pipe lien loop\n",
    "- Two level of Tunings    (1) : piple line to identify the right number of features + unigram/Bigram \n",
    "-                         (2) : Algorithm Hyperparamter tuning for each combination of pipeline loop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
